{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/18lcAtxvFn51-newA-r3ZW1wcimq3PsOT?usp=sharing)"
      ],
      "metadata": {
        "id": "3KbHzf25qbVW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced AI: Transformers for Computer Vision"
      ],
      "metadata": {
        "id": "HCHKaaIQ484A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "N9vBlTLmW9yC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIYdn1woOS1n"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets evaluate gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/jonfernandes/flowers-dataset/raw/main/flower_photos.tgz\n",
        "!tar -xvf flower_photos.tgz\n"
      ],
      "metadata": {
        "id": "ZbuJrEMXNhHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls flower_photos"
      ],
      "metadata": {
        "id": "yl09oLImNtCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset('imagefolder', data_files='https://github.com/jonfernandes/flowers-dataset/raw/main/flower_photos.tgz')\n",
        "ds"
      ],
      "metadata": {
        "id": "XO3XUZXG5Phh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "  display(ds['train'][i]['image'])"
      ],
      "metadata": {
        "id": "zZRYrUL-8GQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = ds['train'].features['label'].names\n",
        "labels"
      ],
      "metadata": {
        "id": "cM-f3dIG7Cu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_train_validation = ds['train'].train_test_split(test_size=0.1, seed=1, shuffle=True)\n",
        "ds_train_validation"
      ],
      "metadata": {
        "id": "aJR68KTJ8GMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_train_validation['validation'] = ds_train_validation.pop('test')\n",
        "ds_train_validation"
      ],
      "metadata": {
        "id": "29XQYliL8GKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds.update(ds_train_validation)\n",
        "ds"
      ],
      "metadata": {
        "id": "KynqLa6V8GH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_train_test = ds['train'].train_test_split(test_size=0.1, seed=1, shuffle=True)\n",
        "ds_train_test"
      ],
      "metadata": {
        "id": "BuWuiO9iIz-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds.update(ds_train_test)\n",
        "ds"
      ],
      "metadata": {
        "id": "afrPYN-hI7Hx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using a pre-trained model without fine-tuning"
      ],
      "metadata": {
        "id": "qBJ54O629Elh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForImageClassification, AutoFeatureExtractor\n",
        "import torch\n",
        "model_id = 'google/vit-base-patch16-224'\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = AutoModelForImageClassification.from_pretrained(model_id).to(device)\n",
        "model.eval()\n"
      ],
      "metadata": {
        "id": "Fxc4qKQQ8GF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_extractor = AutoFeatureExtractor.from_pretrained(model_id)\n",
        "feature_extractor"
      ],
      "metadata": {
        "id": "HSUuSBbw8GD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_image_id = 3\n",
        "one_image = ds['train'][train_image_id]['image']\n",
        "one_image"
      ],
      "metadata": {
        "id": "xnfOjGTQFrWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inp = feature_extractor(images=one_image, return_tensors='pt')\n",
        "inp"
      ],
      "metadata": {
        "id": "8UzxKWx98GBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This will not work if you are using a GPU\n",
        "outp = model(**inp)\n",
        "outp"
      ],
      "metadata": {
        "id": "MDqS38o58F_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inp = feature_extractor(images=one_image, return_tensors='pt').to(device)\n",
        "outp = model(**inp)\n",
        "outp"
      ],
      "metadata": {
        "id": "rw5zMxzkFVS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outp.logits.shape"
      ],
      "metadata": {
        "id": "n3hI5BGdE6mD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.argmax(outp.logits, dim=1)"
      ],
      "metadata": {
        "id": "BFnccBb5E9si"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = torch.argmax(outp.logits, dim=1).item()\n",
        "pred"
      ],
      "metadata": {
        "id": "EuoD66YcNNsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.config"
      ],
      "metadata": {
        "id": "Fvzd-YsBFFp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.id2label[pred]"
      ],
      "metadata": {
        "id": "yy7K6LsVFyWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'daisy' in model.config.label2id"
      ],
      "metadata": {
        "id": "BHHnJh7qoq-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining a model"
      ],
      "metadata": {
        "id": "RJvZfg4-Gh3A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "id2label = {key: value for key, value in enumerate(labels)}\n",
        "id2label"
      ],
      "metadata": {
        "id": "eYRFd6NSHxwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label2id = {value:key for key, value in enumerate(labels)}\n",
        "label2id"
      ],
      "metadata": {
        "id": "85V4W8oEH6YT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForImageClassification.from_pretrained(model_id,\n",
        "                                                        num_labels=len(labels),\n",
        "                                                        id2label=id2label,\n",
        "                                                        label2id=label2id,\n",
        "                                                        ignore_mismatched_sizes=True\n",
        "                                                        )"
      ],
      "metadata": {
        "id": "2hXwllydF6Ct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-processing images"
      ],
      "metadata": {
        "id": "VTW0i2y3JJjy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "\n",
        "from torchvision.transforms import (\n",
        "    Compose,\n",
        "    Normalize,\n",
        "    RandomHorizontalFlip,\n",
        "    RandomResizedCrop,\n",
        "    ToTensor,\n",
        "    Resize,\n",
        "    CenterCrop\n",
        ")"
      ],
      "metadata": {
        "id": "CjFWrtOxJLQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normalize = Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)"
      ],
      "metadata": {
        "id": "-ftmq_p6Jzfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_transform = Compose(\n",
        "    [\n",
        "     RandomResizedCrop(feature_extractor.size),\n",
        "     RandomHorizontalFlip(),\n",
        "     ToTensor(),\n",
        "     normalize\n",
        "    ]\n",
        ")\n",
        "\n",
        "validation_transform = Compose(\n",
        "        [\n",
        "            Resize(feature_extractor.size),\n",
        "            CenterCrop(feature_extractor.size),\n",
        "            ToTensor(),\n",
        "            normalize,\n",
        "        ]\n",
        "    )\n",
        "\n",
        "def train_transform_images(images):\n",
        "  images[\"pixel_values\"] = [train_transform(image.convert(\"RGB\")) for image in images[\"image\"]]\n",
        "  return images\n",
        "\n",
        "def validation_transform_images(images):\n",
        "  images[\"pixel_values\"] = [validation_transform(image.convert(\"RGB\")) for image in images[\"image\"]]\n",
        "  return images"
      ],
      "metadata": {
        "id": "nXlmCLMIJ7Mv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformed_ds = ds.with_transform(train_transform_images)\n",
        "transformed_ds['train'] = ds['train'].with_transform(train_transform_images)\n",
        "transformed_ds['validation'] = ds['validation'].with_transform(validation_transform_images)\n",
        "transformed_ds['test'] = ds['test'].with_transform(validation_transform_images)"
      ],
      "metadata": {
        "id": "1hnwnUBYKAVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A transformed image"
      ],
      "metadata": {
        "id": "UiPdn57OKLmG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_image = ds['train'][train_image_id]['image']\n",
        "sample_image"
      ],
      "metadata": {
        "id": "ywzesa4YKUdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-run cell multiple times\n",
        "import matplotlib.pyplot as plt\n",
        "transformed_sample_image = train_transform(sample_image)\n",
        "plt.imshow(transformed_sample_image.permute(1, 2, 0))"
      ],
      "metadata": {
        "id": "usahzg5WKNT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting images in the correct format"
      ],
      "metadata": {
        "id": "yZmNkpKwmMAO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4-images**"
      ],
      "metadata": {
        "id": "KfmZYJSbnwM3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "four_images = [transformed_ds['train'][i] for i in range(4)]\n",
        "four_images"
      ],
      "metadata": {
        "id": "Ikhrn_hjmYx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(four_images[0]['pixel_values'].shape, four_images[1]['pixel_values'].shape, four_images[2]['pixel_values'].shape, four_images[3]['pixel_values'].shape)"
      ],
      "metadata": {
        "id": "-rLpZUSboBhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "four_images_labels = [image['label'] for image in four_images]\n",
        "four_images_labels"
      ],
      "metadata": {
        "id": "Exn-nl9fob8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "four_images_labels = torch.tensor([image['label'] for image in four_images])\n",
        "four_images_labels"
      ],
      "metadata": {
        "id": "-NPeAgghotvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#should get an error\n",
        "four_images_pixel_values = torch.tensor([image['pixel_values'] for image in four_images])\n",
        "four_images_pixel_values"
      ],
      "metadata": {
        "id": "WRBcWLA-qCn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "four_images_pixel_values = torch.cat([image['pixel_values'] for image in four_images])\n",
        "four_images_pixel_values"
      ],
      "metadata": {
        "id": "Ot6CGsw-qi8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "four_images_pixel_values.shape"
      ],
      "metadata": {
        "id": "mgbAJ_xjquB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "four_images_pixel_values = torch.stack([image['pixel_values'] for image in four_images])\n",
        "four_images_pixel_values.shape"
      ],
      "metadata": {
        "id": "GkzYRPxxq2Cc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def collate_fn(images):\n",
        "  labels = torch.tensor([image['label'] for image in images])\n",
        "  pixel_values = torch.stack([image['pixel_values'] for image in images])\n",
        "  return {'pixel_values': pixel_values, 'labels': labels}\n",
        "\n",
        "train_dataloader = DataLoader(transformed_ds['train'], batch_size=4, collate_fn=collate_fn, shuffle=True)\n",
        "validation_dataloader = DataLoader(transformed_ds['validation'], batch_size=4, collate_fn=collate_fn, shuffle=False)\n",
        "test_dataloader = DataLoader(transformed_ds['test'], batch_size=4, collate_fn=collate_fn, shuffle=False)"
      ],
      "metadata": {
        "id": "haB_6uiSMluh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = next(iter(train_dataloader))\n",
        "\n",
        "for key, value in batch.items():\n",
        "  print(key, value.shape)"
      ],
      "metadata": {
        "id": "TQbcX4_HH9Bq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training arguments"
      ],
      "metadata": {
        "id": "3RGT44H_LvFR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "batch_size=32\n",
        "metric_name = \"accuracy\"\n",
        "model_name = 'vit-base-patch16-224-finetuned-flower'\n",
        "\n",
        "args = TrainingArguments(\n",
        "    model_name,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=3,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=metric_name,\n",
        "    remove_unused_columns=False,\n",
        "    logging_dir='./logs', \n",
        "    push_to_hub=True\n",
        ")"
      ],
      "metadata": {
        "id": "nkBJ9aVoLsz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "1c2sth6RMGXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global credential.helper store"
      ],
      "metadata": {
        "id": "F9i_4VhsvFyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training"
      ],
      "metadata": {
        "id": "qKUq5fSyL26A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the [evaluate documentation](https://huggingface.co/docs/evaluate/a_quick_tour#compute):\n",
        "\n",
        "```\n",
        "metric.compute(\n",
        "          references=..., \n",
        "          predictions=...)\n",
        "```"
      ],
      "metadata": {
        "id": "U14aEwlYgW7c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "import numpy as np\n",
        "\n",
        "metric = evaluate.load('accuracy')\n",
        "\n",
        "def compute_metrics(batch):\n",
        "  return metric.compute(\n",
        "      references=batch.label_ids,\n",
        "      predictions=np.argmax(batch.predictions, axis=1))"
      ],
      "metadata": {
        "id": "moz6njKCKaRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=transformed_ds['train'],\n",
        "    eval_dataset=transformed_ds['validation'],\n",
        "    tokenizer=feature_extractor,\n",
        "    data_collator=collate_fn,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "id": "Ad1YCGbvLsxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs/"
      ],
      "metadata": {
        "id": "pEBGOgpQmuXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n",
        "trainer.save_model()"
      ],
      "metadata": {
        "id": "ime2ojIamL86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate(transformed_ds['train'])"
      ],
      "metadata": {
        "id": "qCvt6q2EUOWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate(transformed_ds['validation'])"
      ],
      "metadata": {
        "id": "F2dA4bSVUT04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate(transformed_ds['test'])"
      ],
      "metadata": {
        "id": "xrqwVgTMTucY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference in notebook"
      ],
      "metadata": {
        "id": "7R1BWYZkMET2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_image = ds['test'][-2]['image']\n",
        "test_image"
      ],
      "metadata": {
        "id": "m9wye5rMLsvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForImageClassification, AutoFeatureExtractor\n",
        "\n",
        "model_id = f'jonathanfernandes/vit-base-patch16-224-finetuned-flower'\n",
        "\n",
        "def classify_image(image):\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  model = AutoModelForImageClassification.from_pretrained(model_id).to(device)\n",
        "  feature_extractor = AutoFeatureExtractor.from_pretrained(model_id)  \n",
        "  inp = feature_extractor(image, return_tensors='pt').to(device)\n",
        "  outp = model(**inp)\n",
        "  pred = torch.argmax(outp.logits, dim=-1).item()\n",
        "  return model.config.id2label[pred]\n",
        "\n",
        "classify_image(test_image)"
      ],
      "metadata": {
        "id": "jMe0EO2ZPwH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "model_id = f'jonathanfernandes/vit-base-patch16-224-finetuned-flower'\n",
        "\n",
        "def classify_image(image):\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  model = AutoModelForImageClassification.from_pretrained(model_id).to(device)\n",
        "  feature_extractor = AutoFeatureExtractor.from_pretrained(model_id)\n",
        "  inp = feature_extractor(image, return_tensors='pt').to(device)\n",
        "  outp = model(**inp)\n",
        "  pred = torch.nn.functional.softmax(outp.logits, dim=-1)\n",
        "  preds = pred[0].cpu().detach().numpy()\n",
        "  confidence = {label: float(preds[i]) for i, label in enumerate(labels)}\n",
        "  return confidence\n",
        "\n",
        "classify_image(test_image)"
      ],
      "metadata": {
        "id": "Zgwv5kzBf675"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "model_id = f'jonathanfernandes/vit-base-patch16-224-finetuned-flower'\n",
        "feature_extractor = AutoFeatureExtractor.from_pretrained(model_id)\n",
        "\n",
        "image_classifier = pipeline('image-classification', model=model_id, feature_extractor=feature_extractor)\n",
        "image_classifier(test_image)"
      ],
      "metadata": {
        "id": "JAU2ju83nCrh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "help(pipeline)"
      ],
      "metadata": {
        "id": "zPKimYQa14lD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference on your phone using Gradio"
      ],
      "metadata": {
        "id": "Nsz5obyzNzDd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/jonfernandes/Advanced_AI_Transformers_for_Computer_Vision/raw/main/flower-1.jpg\n",
        "!wget https://github.com/jonfernandes/Advanced_AI_Transformers_for_Computer_Vision/raw/main/flower-2.jpeg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQrwkJnP1Cup",
        "outputId": "4daf1ef4-a456-4316-db4f-68940c3c625d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-11-15 07:24:27--  https://github.com/jonfernandes/Advanced_AI_Transformers_for_Computer_Vision/raw/main/flower-1.jpg\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/jonfernandes/Advanced_AI_Transformers_for_Computer_Vision/main/flower-1.jpg [following]\n",
            "--2022-11-15 07:24:27--  https://raw.githubusercontent.com/jonfernandes/Advanced_AI_Transformers_for_Computer_Vision/main/flower-1.jpg\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 712963 (696K) [image/jpeg]\n",
            "Saving to: ‘flower-1.jpg’\n",
            "\n",
            "flower-1.jpg        100%[===================>] 696.25K  --.-KB/s    in 0.007s  \n",
            "\n",
            "2022-11-15 07:24:27 (100 MB/s) - ‘flower-1.jpg’ saved [712963/712963]\n",
            "\n",
            "--2022-11-15 07:24:27--  https://github.com/jonfernandes/Advanced_AI_Transformers_for_Computer_Vision/raw/main/flower-2.jpeg\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/jonfernandes/Advanced_AI_Transformers_for_Computer_Vision/main/flower-2.jpeg [following]\n",
            "--2022-11-15 07:24:28--  https://raw.githubusercontent.com/jonfernandes/Advanced_AI_Transformers_for_Computer_Vision/main/flower-2.jpeg\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 560399 (547K) [image/jpeg]\n",
            "Saving to: ‘flower-2.jpeg’\n",
            "\n",
            "flower-2.jpeg       100%[===================>] 547.26K  --.-KB/s    in 0.005s  \n",
            "\n",
            "2022-11-15 07:24:28 (117 MB/s) - ‘flower-2.jpeg’ saved [560399/560399]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4vhAjzhz3ly",
        "outputId": "55df516e-7658-4796-802a-7450f64f0f83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 448164\n",
            "drwxr-xr-x 2 root   root      4096 Nov 15 07:17 flagged\n",
            "-rw-r--r-- 1 root   root    712963 Nov 15 07:24 flower-1.jpg\n",
            "-rw-r--r-- 1 root   root    560399 Nov 15 07:24 flower-2.jpeg\n",
            "drwxr-x--- 7 270850 5000      4096 Feb 10  2016 flower_photos\n",
            "-rw-r--r-- 1 root   root 228813984 Nov 15 05:46 flower_photos.tgz\n",
            "-rw-r--r-- 1 root   root 228813984 Nov 15 06:19 flower_photos.tgz.1\n",
            "drwxr-xr-x 1 root   root      4096 Nov 11 14:32 sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForImageClassification, AutoFeatureExtractor\n",
        "import gradio as gr\n",
        "\n",
        "model_id = f'jonathanfernandes/vit-base-patch16-224-finetuned-flower'\n",
        "\n",
        "def classify_image(image):\n",
        "  model = AutoModelForImageClassification.from_pretrained(model_id)\n",
        "  feature_extractor = AutoFeatureExtractor.from_pretrained(model_id)\n",
        "  inp = feature_extractor(image, return_tensors='pt')\n",
        "  outp = model(**inp)\n",
        "  pred = torch.nn.functional.softmax(outp.logits, dim=-1)\n",
        "  preds = pred[0].cpu().detach().numpy()\n",
        "  confidence = {label: float(preds[i]) for i, label in enumerate(labels)}\n",
        "  return confidence\n",
        "\n",
        "interface = gr.Interface(fn=classify_image, \n",
        "                         inputs='image', \n",
        "                         examples=['flower-1.jpg', 'flower-2.jpeg'],\n",
        "                         outputs='label').launch(debug=True, share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 608
        },
        "id": "6FMoxQU4sBf7",
        "outputId": "bfe2c214-5c57-4d7c-b3ea-9413d0e093e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://c269db8febe09b89.gradio.app\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://c269db8febe09b89.gradio.app\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "qBJ54O629Elh",
        "RJvZfg4-Gh3A",
        "VTW0i2y3JJjy",
        "UiPdn57OKLmG",
        "3RGT44H_LvFR",
        "qKUq5fSyL26A",
        "7R1BWYZkMET2"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}